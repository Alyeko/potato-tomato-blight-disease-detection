{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains...","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:07:18.177568Z","iopub.execute_input":"2022-07-31T16:07:18.177946Z","iopub.status.idle":"2022-07-31T16:07:31.587264Z","shell.execute_reply.started":"2022-07-31T16:07:18.177855Z","shell.execute_reply":"2022-07-31T16:07:31.586143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os                                      # for working with files\nimport sys\nimport shap                                    # for checking feature importances\nimport torch                                   # Pytorch module \nimport shutil\nimport optuna\nimport warnings\nimport numpy as np                             # for numerical computationss\nimport pandas as pd                            # for working with dataframes\nimport torch.nn as nn                          # for creating  neural networks\nfrom PIL import Image                          # for checking images\nimport matplotlib.pyplot as plt                # for plotting informations on graph and images using tensors\nimport torch.nn.functional as F                # for functions for calculating loss\nfrom torchsummary import summary               # for getting the summary of our model\nfrom torchvision.utils import make_grid        # for data checking\nfrom torch.utils.data import DataLoader        # for dataloaders \nimport torchvision.transforms as transforms    # for transforming images into tensors \nfrom torchvision.datasets import ImageFolder   # for working with classes and images\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:07:31.590567Z","iopub.execute_input":"2022-07-31T16:07:31.590975Z","iopub.status.idle":"2022-07-31T16:07:35.965672Z","shell.execute_reply.started":"2022-07-31T16:07:31.590942Z","shell.execute_reply":"2022-07-31T16:07:35.964707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration!","metadata":{}},{"cell_type":"code","source":"os.listdir('/kaggle/input/dataset/idata/Image Dataset/ImageDataset/')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:46.057088Z","iopub.execute_input":"2022-07-31T16:08:46.057642Z","iopub.status.idle":"2022-07-31T16:08:46.081082Z","shell.execute_reply.started":"2022-07-31T16:08:46.057607Z","shell.execute_reply":"2022-07-31T16:08:46.080121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/dataset/idata/Image Dataset/ImageDataset/'","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:46.845668Z","iopub.execute_input":"2022-07-31T16:08:46.846365Z","iopub.status.idle":"2022-07-31T16:08:46.851461Z","shell.execute_reply.started":"2022-07-31T16:08:46.846328Z","shell.execute_reply":"2022-07-31T16:08:46.850069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"Number of image directories are {len(os.listdir(data_fpath))+len(os.listdir('/kaggle/input/newds/ImageDataset_new/ImageDataset_new/'))}\\n\")\nprint('Number of unique plants are 2, potato and tomato\\n')\nprint('Number of diseases are 4, early and late blight disease for tomato, early and late blight for potato\\n')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:47.822055Z","iopub.execute_input":"2022-07-31T16:08:47.822430Z","iopub.status.idle":"2022-07-31T16:08:47.828503Z","shell.execute_reply.started":"2022-07-31T16:08:47.822398Z","shell.execute_reply":"2022-07-31T16:08:47.827158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:48.502978Z","iopub.execute_input":"2022-07-31T16:08:48.503866Z","iopub.status.idle":"2022-07-31T16:08:48.510380Z","shell.execute_reply.started":"2022-07-31T16:08:48.503827Z","shell.execute_reply":"2022-07-31T16:08:48.509167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = data_dir + \"train/\"\nvalid_dir = data_dir + \"valid/\"\n# test_dir\ndiseases_tr = os.listdir(train_dir)\ndiseases_va = os.listdir(valid_dir)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:48.973427Z","iopub.execute_input":"2022-07-31T16:08:48.975992Z","iopub.status.idle":"2022-07-31T16:08:48.993300Z","shell.execute_reply.started":"2022-07-31T16:08:48.975953Z","shell.execute_reply":"2022-07-31T16:08:48.992279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dir","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:49.594705Z","iopub.execute_input":"2022-07-31T16:08:49.595485Z","iopub.status.idle":"2022-07-31T16:08:49.602950Z","shell.execute_reply.started":"2022-07-31T16:08:49.595446Z","shell.execute_reply":"2022-07-31T16:08:49.601636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diseases_tr","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:50.190723Z","iopub.execute_input":"2022-07-31T16:08:50.191567Z","iopub.status.idle":"2022-07-31T16:08:50.199423Z","shell.execute_reply.started":"2022-07-31T16:08:50.191516Z","shell.execute_reply":"2022-07-31T16:08:50.198369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plants = []\nNumberOfDiseases = 0\nfor plant in diseases_tr:\n    if plant.split('___')[0] not in plants:\n        plants.append(plant.split('___')[0])\n    if plant.split('_')[1] != 'healthy':\n        NumberOfDiseases += 1","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:50.826599Z","iopub.execute_input":"2022-07-31T16:08:50.827073Z","iopub.status.idle":"2022-07-31T16:08:50.838977Z","shell.execute_reply.started":"2022-07-31T16:08:50.827032Z","shell.execute_reply":"2022-07-31T16:08:50.837821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of images for each clas in the training data\nnums_train = {}\nfor folder in sorted(os.listdir(f\"{data_dir}/train\")):\n    nums_train[folder] = len(os.listdir(f\"/{data_dir}/train/{folder}\"))\n    \n# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n\nimg_per_training_class = pd.DataFrame(nums_train.values(), index=nums_train.keys(), columns=[\"no. of images\"])\nimg_per_training_class","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:51.404351Z","iopub.execute_input":"2022-07-31T16:08:51.404970Z","iopub.status.idle":"2022-07-31T16:08:53.935469Z","shell.execute_reply.started":"2022-07-31T16:08:51.404932Z","shell.execute_reply":"2022-07-31T16:08:53.934532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of images for each clas in the training data\nnums_valid = {}\nfor folder in sorted(os.listdir(f\"{data_dir}/valid\")):\n    nums_valid[folder] = len(os.listdir(f\"{data_dir}/valid/{folder}\"))\n    \n# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n\nimg_per_valid_class = pd.DataFrame(nums_valid.values(), index=nums_valid.keys(), columns=[\"no. of images\"])\nimg_per_valid_class","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:53.937510Z","iopub.execute_input":"2022-07-31T16:08:53.937914Z","iopub.status.idle":"2022-07-31T16:08:55.449066Z","shell.execute_reply.started":"2022-07-31T16:08:53.937876Z","shell.execute_reply":"2022-07-31T16:08:55.447898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting number of images available for each class\nindex = [n for n in range(6)]\nplt.figure(figsize=(20, 5))\nplt.bar(index, [n for n in nums_train.values()], color='#8528B0')\nplt.xlabel('Classes', fontsize=15)\nplt.ylabel('No of images available', fontsize=15)\nplt.xticks(index, [key for key in nums_train.keys()], fontsize=15, rotation=90)\nplt.title('Images per class for training dataset')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:08:59.076817Z","iopub.execute_input":"2022-07-31T16:08:59.077487Z","iopub.status.idle":"2022-07-31T16:08:59.328351Z","shell.execute_reply.started":"2022-07-31T16:08:59.077450Z","shell.execute_reply":"2022-07-31T16:08:59.327485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting number of images available for each class\nindex = [n for n in range(6)]\nplt.figure(figsize=(20, 5))\nplt.bar(index, [n for n in nums_valid.values()], color='#8528B0')\nplt.xlabel('Classes', fontsize=15)\nplt.ylabel('No of images available', fontsize=15)\nplt.xticks(index, [key for key in nums_valid.keys()], fontsize=15, rotation=90)\nplt.title('Images per class for validation dataset')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:00.886660Z","iopub.execute_input":"2022-07-31T16:09:00.887570Z","iopub.status.idle":"2022-07-31T16:09:01.103721Z","shell.execute_reply.started":"2022-07-31T16:09:00.887533Z","shell.execute_reply":"2022-07-31T16:09:01.102678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"The data has already been augmented. see https://github.com/Alyeko/potato-tomato-blight-disease-detection","metadata":{"execution":{"iopub.status.busy":"2022-07-08T19:53:01.18532Z","iopub.execute_input":"2022-07-08T19:53:01.185753Z","iopub.status.idle":"2022-07-08T19:53:01.192473Z","shell.execute_reply.started":"2022-07-08T19:53:01.185709Z","shell.execute_reply":"2022-07-08T19:53:01.191217Z"}}},{"cell_type":"markdown","source":"### Images available for training","metadata":{}},{"cell_type":"code","source":"n_train = 0\nfor value in nums_train.values():\n    n_train += value\nprint(f\"There are {n_train} images for training\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:10.868591Z","iopub.execute_input":"2022-07-31T16:09:10.869257Z","iopub.status.idle":"2022-07-31T16:09:10.875289Z","shell.execute_reply.started":"2022-07-31T16:09:10.869218Z","shell.execute_reply":"2022-07-31T16:09:10.874297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_valid = 0\nfor value in nums_valid.values():\n    n_valid += value\nprint(f\"There are {n_valid} images for validation\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:11.517672Z","iopub.execute_input":"2022-07-31T16:09:11.518542Z","iopub.status.idle":"2022-07-31T16:09:11.524428Z","shell.execute_reply.started":"2022-07-31T16:09:11.518504Z","shell.execute_reply":"2022-07-31T16:09:11.523380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking if here are non img files in the training data folder\n","metadata":{}},{"cell_type":"code","source":"folds = [folder for folder in os.listdir(train_dir)]\nfolds","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:12.393360Z","iopub.execute_input":"2022-07-31T16:09:12.393808Z","iopub.status.idle":"2022-07-31T16:09:12.407676Z","shell.execute_reply.started":"2022-07-31T16:09:12.393768Z","shell.execute_reply":"2022-07-31T16:09:12.406807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in folds:\n    for img in os.listdir(train_dir+i):\n        if not img.endswith('.JPG'):\n            print('yes!')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:12.859229Z","iopub.execute_input":"2022-07-31T16:09:12.859629Z","iopub.status.idle":"2022-07-31T16:09:12.876064Z","shell.execute_reply.started":"2022-07-31T16:09:12.859595Z","shell.execute_reply":"2022-07-31T16:09:12.875135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in folds:\n    for img in os.listdir(valid_dir+i):\n        if not img.endswith('.JPG'):\n            print('yes!')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:13.330879Z","iopub.execute_input":"2022-07-31T16:09:13.331569Z","iopub.status.idle":"2022-07-31T16:09:13.343123Z","shell.execute_reply.started":"2022-07-31T16:09:13.331530Z","shell.execute_reply":"2022-07-31T16:09:13.341957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:13.818014Z","iopub.execute_input":"2022-07-31T16:09:13.819017Z","iopub.status.idle":"2022-07-31T16:09:13.825837Z","shell.execute_reply.started":"2022-07-31T16:09:13.818968Z","shell.execute_reply":"2022-07-31T16:09:13.824683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {len(os.listdir('/kaggle/input/dataset/idata/Image Dataset/test_data/test'))} images for test\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:14.490542Z","iopub.execute_input":"2022-07-31T16:09:14.491633Z","iopub.status.idle":"2022-07-31T16:09:14.890446Z","shell.execute_reply.started":"2022-07-31T16:09:14.491583Z","shell.execute_reply":"2022-07-31T16:09:14.889360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training dir: {os.listdir('/kaggle/input/dataset/idata/Image Dataset/ImageDataset/')}\")\nprint(f\"All: {os.listdir('/kaggle/input/dataset/idata/Image Dataset')}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:14.915187Z","iopub.execute_input":"2022-07-31T16:09:14.917023Z","iopub.status.idle":"2022-07-31T16:09:14.925103Z","shell.execute_reply.started":"2022-07-31T16:09:14.916975Z","shell.execute_reply":"2022-07-31T16:09:14.923923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir = '/kaggle/input/dataset/idata/Image Dataset/test_data/'\n# print(f\"There are {len(os.listdir('/kaggle/input/newds/ImageDataset_new/ImageDataset_new/test_data'))} images for training\")\nos.listdir(test_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:15.502803Z","iopub.execute_input":"2022-07-31T16:09:15.503893Z","iopub.status.idle":"2022-07-31T16:09:15.512332Z","shell.execute_reply.started":"2022-07-31T16:09:15.503845Z","shell.execute_reply":"2022-07-31T16:09:15.511171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img in os.listdir(test_dir+'test'):\n        if not img.endswith('.JPG'):\n            print('Yes! I knew it!')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:15.924065Z","iopub.execute_input":"2022-07-31T16:09:15.926072Z","iopub.status.idle":"2022-07-31T16:09:15.933400Z","shell.execute_reply.started":"2022-07-31T16:09:15.926021Z","shell.execute_reply":"2022-07-31T16:09:15.932301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation for training ","metadata":{}},{"cell_type":"markdown","source":"# datasets for validation and training\ntrain = ImageFolder(train_dir, \n                    transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))\n\nprint(train, '\\n')\nvalid = ImageFolder(valid_dir,\n                      transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]))\nprint(valid, '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:26:20.076228Z","iopub.execute_input":"2022-07-10T11:26:20.076668Z","iopub.status.idle":"2022-07-10T11:26:22.591953Z","shell.execute_reply.started":"2022-07-10T11:26:20.076626Z","shell.execute_reply":"2022-07-10T11:26:22.591119Z"}}},{"cell_type":"markdown","source":"Next, after loading the data, we need to transform the pixel values of each image (0-255) to 0-1 as neural networks works quite good with normalized data. The entire array of pixel values is converted to torch [tensor](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html#:~:text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.) and then divided by 255.\nIf you are not familiar why normalizing inputs help neural network, read [this](https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d) post.","metadata":{}},{"cell_type":"code","source":"print(train_dir)\nprint(valid_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:17.607224Z","iopub.execute_input":"2022-07-31T16:09:17.608309Z","iopub.status.idle":"2022-07-31T16:09:17.614348Z","shell.execute_reply.started":"2022-07-31T16:09:17.608263Z","shell.execute_reply":"2022-07-31T16:09:17.612996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datasets for validation and training\ntrain = ImageFolder(train_dir, transform=transforms.Compose(\n                                        [transforms.Resize([256, 256]),\n                                         transforms.ToTensor()]))\n\nvalid = ImageFolder(valid_dir, transform=transforms.Compose(\n                                        [transforms.Resize([256, 256]),\n                                         transforms.ToTensor()]))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:18.268786Z","iopub.execute_input":"2022-07-31T16:09:18.269813Z","iopub.status.idle":"2022-07-31T16:09:30.664066Z","shell.execute_reply.started":"2022-07-31T16:09:18.269768Z","shell.execute_reply":"2022-07-31T16:09:30.663080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Image shape\nimg, label = train[4590]\nprint(img.shape, label)\n\nimg, label = train[0]\nprint(img.shape, label)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:30.666196Z","iopub.execute_input":"2022-07-31T16:09:30.666567Z","iopub.status.idle":"2022-07-31T16:09:30.691725Z","shell.execute_reply.started":"2022-07-31T16:09:30.666532Z","shell.execute_reply":"2022-07-31T16:09:30.690735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the shape (3, 256 256) of the image. 3 is the number of channels (RGB) and 256 x 256 is the width and height of the image","metadata":{}},{"cell_type":"code","source":"len(train.classes) #multiclass classification with 6 classes","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:30.693010Z","iopub.execute_input":"2022-07-31T16:09:30.693502Z","iopub.status.idle":"2022-07-31T16:09:30.699994Z","shell.execute_reply.started":"2022-07-31T16:09:30.693474Z","shell.execute_reply":"2022-07-31T16:09:30.698878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for checking some images from training dataset\ndef show_image(image, label):\n    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n    plt.imshow(image.permute(1, 2, 0))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:34.575242Z","iopub.execute_input":"2022-07-31T16:09:34.575677Z","iopub.status.idle":"2022-07-31T16:09:34.582167Z","shell.execute_reply.started":"2022-07-31T16:09:34.575642Z","shell.execute_reply":"2022-07-31T16:09:34.581017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the seed value\nrandom_seed = 7\ntorch.manual_seed(random_seed)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:34.946334Z","iopub.execute_input":"2022-07-31T16:09:34.946684Z","iopub.status.idle":"2022-07-31T16:09:34.956639Z","shell.execute_reply.started":"2022-07-31T16:09:34.946653Z","shell.execute_reply":"2022-07-31T16:09:34.955708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[10000])","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:35.478349Z","iopub.execute_input":"2022-07-31T16:09:35.478719Z","iopub.status.idle":"2022-07-31T16:09:35.682354Z","shell.execute_reply.started":"2022-07-31T16:09:35.478680Z","shell.execute_reply":"2022-07-31T16:09:35.681428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[6580])","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:36.097339Z","iopub.execute_input":"2022-07-31T16:09:36.097710Z","iopub.status.idle":"2022-07-31T16:09:36.309586Z","shell.execute_reply.started":"2022-07-31T16:09:36.097672Z","shell.execute_reply":"2022-07-31T16:09:36.308681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[1000])","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:49.745046Z","iopub.execute_input":"2022-07-31T16:09:49.745443Z","iopub.status.idle":"2022-07-31T16:09:49.955653Z","shell.execute_reply.started":"2022-07-31T16:09:49.745410Z","shell.execute_reply":"2022-07-31T16:09:49.954761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(*train[5000])","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:50.394885Z","iopub.execute_input":"2022-07-31T16:09:50.395525Z","iopub.status.idle":"2022-07-31T16:09:50.604846Z","shell.execute_reply.started":"2022-07-31T16:09:50.395491Z","shell.execute_reply":"2022-07-31T16:09:50.603792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train, '\\n')\nprint(valid)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:50.714151Z","iopub.execute_input":"2022-07-31T16:09:50.714514Z","iopub.status.idle":"2022-07-31T16:09:50.722833Z","shell.execute_reply.started":"2022-07-31T16:09:50.714483Z","shell.execute_reply":"2022-07-31T16:09:50.719029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataLoaders for training and validation\n# setting the batch size\nbatch_size = 32\ntrain_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nvalid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:51.360364Z","iopub.execute_input":"2022-07-31T16:09:51.360736Z","iopub.status.idle":"2022-07-31T16:09:51.367336Z","shell.execute_reply.started":"2022-07-31T16:09:51.360703Z","shell.execute_reply":"2022-07-31T16:09:51.366091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to show a batch of training instances\ndef show_batch(data):\n    for images, labels in data:\n        fig, ax = plt.subplots(figsize=(30, 30))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n        break\n        \n# Images for first batch of training\nshow_batch(train_dl) ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:52.033895Z","iopub.execute_input":"2022-07-31T16:09:52.034272Z","iopub.status.idle":"2022-07-31T16:09:57.417799Z","shell.execute_reply.started":"2022-07-31T16:09:52.034241Z","shell.execute_reply":"2022-07-31T16:09:57.413844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🏗️ Modelling 🏗️","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:57.419992Z","iopub.execute_input":"2022-07-31T16:09:57.420976Z","iopub.status.idle":"2022-07-31T16:09:57.426310Z","shell.execute_reply.started":"2022-07-31T16:09:57.420935Z","shell.execute_reply":"2022-07-31T16:09:57.424728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for moving data into GPU (if available)\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available:\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n\n# for moving data to device (CPU or GPU)\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n# for loading in the device (GPU if available else CPU)\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl:\n            yield to_device(b, self.device)\n        \n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:57.427991Z","iopub.execute_input":"2022-07-31T16:09:57.428340Z","iopub.status.idle":"2022-07-31T16:09:57.442293Z","shell.execute_reply.started":"2022-07-31T16:09:57.428308Z","shell.execute_reply":"2022-07-31T16:09:57.441238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:09:57.444263Z","iopub.execute_input":"2022-07-31T16:09:57.445324Z","iopub.status.idle":"2022-07-31T16:09:57.453226Z","shell.execute_reply.started":"2022-07-31T16:09:57.445240Z","shell.execute_reply":"2022-07-31T16:09:57.451781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Moving data into GPU\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:01.757817Z","iopub.execute_input":"2022-07-31T16:10:01.760365Z","iopub.status.idle":"2022-07-31T16:10:01.765926Z","shell.execute_reply.started":"2022-07-31T16:10:01.760325Z","shell.execute_reply":"2022-07-31T16:10:01.764815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x # ReLU can be applied before or after adding the input","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:02.195550Z","iopub.execute_input":"2022-07-31T16:10:02.196194Z","iopub.status.idle":"2022-07-31T16:10:02.203955Z","shell.execute_reply.started":"2022-07-31T16:10:02.196158Z","shell.execute_reply":"2022-07-31T16:10:02.202985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for calculating the accuracy\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n\n# base class for the model\nclass ImageClassificationBase(nn.Module):\n    \n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)                   # Generate prediction\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        acc = accuracy(out, labels)          # Calculate accuracy\n        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x[\"val_loss\"] for x in outputs]\n        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  \n        epoch_accuracy = torch.stack(batch_accuracy).mean()\n        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy} # Combine accuracies\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:02.595209Z","iopub.execute_input":"2022-07-31T16:10:02.595833Z","iopub.status.idle":"2022-07-31T16:10:02.608326Z","shell.execute_reply.started":"2022-07-31T16:10:02.595795Z","shell.execute_reply":"2022-07-31T16:10:02.607134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Architecture for training\n\n# convolution block with BatchNormalization\ndef ConvBlock(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n             nn.BatchNorm2d(out_channels),\n             nn.ReLU(inplace=True)]\n    if pool:\n        layers.append(nn.MaxPool2d(4))\n    return nn.Sequential(*layers)\n\n\n# resnet architecture \nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_diseases):\n        super().__init__()\n        \n        self.conv1 = ConvBlock(in_channels, 64)\n        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n        \n        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n                                       nn.Flatten(),\n                                       nn.Linear(512, num_diseases), \n                                       nn.Softmax(dim=1))\n        \n    def forward(self, xb): # xb is the loaded batch\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out        ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:03.072162Z","iopub.execute_input":"2022-07-31T16:10:03.072843Z","iopub.status.idle":"2022-07-31T16:10:03.086280Z","shell.execute_reply.started":"2022-07-31T16:10:03.072801Z","shell.execute_reply":"2022-07-31T16:10:03.085331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the model and moving it to the GPU\nmodel = to_device(ResNet9(3, len(train.classes)), device) \nmodel","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:03.522498Z","iopub.execute_input":"2022-07-31T16:10:03.523291Z","iopub.status.idle":"2022-07-31T16:10:03.599958Z","shell.execute_reply.started":"2022-07-31T16:10:03.523242Z","shell.execute_reply":"2022-07-31T16:10:03.598727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting summary of the model\nINPUT_SHAPE = (3, 256, 256)\nprint(summary(model.cuda(), (INPUT_SHAPE)))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:04.631071Z","iopub.execute_input":"2022-07-31T16:10:04.631453Z","iopub.status.idle":"2022-07-31T16:10:11.126208Z","shell.execute_reply.started":"2022-07-31T16:10:04.631422Z","shell.execute_reply":"2022-07-31T16:10:11.124077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for training\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \n\ndef fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n                grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    \n    # scheduler for one cycle learniing rate\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n    \n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n                \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # recording and updating learning rates\n            lrs.append(get_lr(optimizer))\n            sched.step()\n            \n    \n        # validation\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n        \n    return history\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:11.128149Z","iopub.execute_input":"2022-07-31T16:10:11.128445Z","iopub.status.idle":"2022-07-31T16:10:11.141202Z","shell.execute_reply.started":"2022-07-31T16:10:11.128418Z","shell.execute_reply":"2022-07-31T16:10:11.140229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory = [evaluate(model, valid_dl)]\nhistory","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:11:14.234143Z","iopub.execute_input":"2022-07-31T16:11:14.234541Z","iopub.status.idle":"2022-07-31T16:11:32.203684Z","shell.execute_reply.started":"2022-07-31T16:11:14.234508Z","shell.execute_reply":"2022-07-31T16:11:32.202538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------","metadata":{}},{"cell_type":"markdown","source":"### Hyperparamter tuning with optuna","metadata":{}},{"cell_type":"markdown","source":"Code for hyperparameter tuning adapted from https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837","metadata":{}},{"cell_type":"code","source":"len(valid_dl)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:16.911669Z","iopub.execute_input":"2022-07-31T16:10:16.912063Z","iopub.status.idle":"2022-07-31T16:10:16.919037Z","shell.execute_reply.started":"2022-07-31T16:10:16.912024Z","shell.execute_reply":"2022-07-31T16:10:16.917999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(param, model, train_dl, valid_dl):\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = getattr(torch.optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n\n    if use_cuda:\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    for epoch_num in range(EPOCHS):\n            total_acc_train = 0\n            total_loss_train = 0\n            for train_input, train_label in train_dl:\n                train_label = train_label.to(device)\n                train_input = train_input.to(device)\n\n                output = model(train_input.float())\n                \n                batch_loss = criterion(output, train_label.long())\n                total_loss_train += batch_loss.item()\n                \n                acc = (output.argmax(dim=1) == train_label).sum().item()\n                total_acc_train += acc\n\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            \n            total_acc_val = 0\n            total_loss_val = 0\n            with torch.no_grad():\n                num_diff_val_accuracies = []\n                for val_input, val_label in valid_dl:\n                    val_label = val_label.to(device)\n                    val_input = val_input.to(device)\n\n                    output = model(val_input.float())\n\n                    batch_loss = criterion(output, val_label.long())\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = (output.argmax(dim=1) == val_label).sum().item()\n                    num_diff_val_accuracies.append(acc)\n                    total_acc_val += acc\n                    \n                    \n            accuracy = total_acc_val/len(num_diff_val_accuracies)                \n    return accuracy\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:16.920777Z","iopub.execute_input":"2022-07-31T16:10:16.921461Z","iopub.status.idle":"2022-07-31T16:10:16.936721Z","shell.execute_reply.started":"2022-07-31T16:10:16.921418Z","shell.execute_reply":"2022-07-31T16:10:16.935792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    params = {'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n                    'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"]),\n                 'weight_decay': trial.suggest_loguniform('weight_decay', 1e-4, 1e-1),\n                    'grad_clip': trial.suggest_float('grad_clip', 0.1, 0.4),\n                      'epochs' : trial.suggest_int('epochs', 2, 7)\n              }\n            \n    accuracy = train_and_evaluate(params, model,  train_dl, valid_dl)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:10:16.940394Z","iopub.execute_input":"2022-07-31T16:10:16.941002Z","iopub.status.idle":"2022-07-31T16:10:16.951215Z","shell.execute_reply.started":"2022-07-31T16:10:16.940967Z","shell.execute_reply":"2022-07-31T16:10:16.950136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.create_study?","metadata":{"execution":{"iopub.status.busy":"2022-07-30T13:39:55.005003Z","iopub.execute_input":"2022-07-30T13:39:55.006211Z","iopub.status.idle":"2022-07-30T13:39:55.068738Z","shell.execute_reply.started":"2022-07-30T13:39:55.006160Z","shell.execute_reply":"2022-07-30T13:39:55.067770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nEPOCHS = 8\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=8)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:24:36.840037Z","iopub.execute_input":"2022-07-30T12:24:36.840447Z","iopub.status.idle":"2022-07-30T12:24:58.843196Z","shell.execute_reply.started":"2022-07-30T12:24:36.840407Z","shell.execute_reply":"2022-07-30T12:24:58.842147Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_trial = study.best_trial\n\nfor key, value in best_trial.params.items():\n    print(\"{}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:27:07.807429Z","iopub.execute_input":"2022-07-27T16:27:07.807870Z","iopub.status.idle":"2022-07-27T16:27:07.816773Z","shell.execute_reply.started":"2022-07-27T16:27:07.807838Z","shell.execute_reply":"2022-07-27T16:27:07.815538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the hyperparameter tuning process","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_intermediate_values(study)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:27:13.774923Z","iopub.execute_input":"2022-07-27T16:27:13.775326Z","iopub.status.idle":"2022-07-27T16:27:14.182612Z","shell.execute_reply.started":"2022-07-27T16:27:13.775293Z","shell.execute_reply":"2022-07-27T16:27:14.181413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)  #visualizing the tuning history","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:27:25.522756Z","iopub.execute_input":"2022-07-27T16:27:25.523589Z","iopub.status.idle":"2022-07-27T16:27:25.547714Z","shell.execute_reply.started":"2022-07-27T16:27:25.523438Z","shell.execute_reply":"2022-07-27T16:27:25.546530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(study) #visualizing the parameter importances","metadata":{"execution":{"iopub.status.busy":"2022-07-27T16:27:36.407291Z","iopub.execute_input":"2022-07-27T16:27:36.407742Z","iopub.status.idle":"2022-07-27T16:27:36.451038Z","shell.execute_reply.started":"2022-07-27T16:27:36.407704Z","shell.execute_reply":"2022-07-27T16:27:36.449547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To-Do!","metadata":{"execution":{"iopub.status.busy":"2022-07-27T14:38:33.714931Z","iopub.execute_input":"2022-07-27T14:38:33.715345Z","iopub.status.idle":"2022-07-27T14:38:33.722916Z","shell.execute_reply.started":"2022-07-27T14:38:33.715309Z","shell.execute_reply":"2022-07-27T14:38:33.721072Z"}}},{"cell_type":"markdown","source":"1. change hyper params and see how accuracy changes\n2. import an image, export with dpi and see the difference\n3. You have too many bullet points in overleaf, make them paragraph-y","metadata":{}},{"cell_type":"markdown","source":"---------------------","metadata":{"execution":{"iopub.status.busy":"2022-07-26T05:03:50.519722Z","iopub.execute_input":"2022-07-26T05:03:50.520435Z","iopub.status.idle":"2022-07-26T05:03:50.555171Z","shell.execute_reply.started":"2022-07-26T05:03:50.520370Z","shell.execute_reply":"2022-07-26T05:03:50.551821Z"}}},{"cell_type":"code","source":"epochs = 5\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.SGD\n#opt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:11:47.716464Z","iopub.execute_input":"2022-07-31T16:11:47.716925Z","iopub.status.idle":"2022-07-31T16:11:47.723822Z","shell.execute_reply.started":"2022-07-31T16:11:47.716884Z","shell.execute_reply":"2022-07-31T16:11:47.721906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_OneCycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=1e-4, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:11:49.012307Z","iopub.execute_input":"2022-07-31T16:11:49.012794Z","iopub.status.idle":"2022-07-31T16:19:45.982426Z","shell.execute_reply.started":"2022-07-31T16:11:49.012736Z","shell.execute_reply":"2022-07-31T16:19:45.981143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_accuracy'] for x in history]\n    plt.grid(color='#EAE4E3')\n    plt.plot(accuracies, '-x', color='black')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x.get('val_loss').cpu().numpy() for x in history] #[x['val_loss'] for x in history]\n    plt.grid(color='#EAE4E3')\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.grid(color='#EAE4E3')\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:06.026224Z","iopub.execute_input":"2022-07-31T16:20:06.026621Z","iopub.status.idle":"2022-07-31T16:20:06.037422Z","shell.execute_reply.started":"2022-07-31T16:20:06.026586Z","shell.execute_reply":"2022-07-31T16:20:06.036333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation accuracy\nplot_accuracies(history)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:08.107580Z","iopub.execute_input":"2022-07-31T16:20:08.108567Z","iopub.status.idle":"2022-07-31T16:20:08.284591Z","shell.execute_reply.started":"2022-07-31T16:20:08.108519Z","shell.execute_reply":"2022-07-31T16:20:08.283626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation loss\nplot_losses(history)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:09.873805Z","iopub.execute_input":"2022-07-31T16:20:09.877090Z","iopub.status.idle":"2022-07-31T16:20:10.136576Z","shell.execute_reply.started":"2022-07-31T16:20:09.877041Z","shell.execute_reply":"2022-07-31T16:20:10.135619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning Rate overtime\nplot_lrs(history)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:11.829316Z","iopub.execute_input":"2022-07-31T16:20:11.830023Z","iopub.status.idle":"2022-07-31T16:20:12.020606Z","shell.execute_reply.started":"2022-07-31T16:20:11.829984Z","shell.execute_reply":"2022-07-31T16:20:12.019715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir('../input/imagedataset/ImageDataset/test_data')\n# os.listdir('../test')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:23.208168Z","iopub.execute_input":"2022-07-31T16:20:23.208906Z","iopub.status.idle":"2022-07-31T16:20:23.213169Z","shell.execute_reply.started":"2022-07-31T16:20:23.208866Z","shell.execute_reply":"2022-07-31T16:20:23.212078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###Creating a new test dir bcause there was an svn file or folder found in the test dir\nos.mkdir('../test_data')\nos.mkdir('../test_data/test')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:24.114712Z","iopub.execute_input":"2022-07-31T16:20:24.115120Z","iopub.status.idle":"2022-07-31T16:20:24.121569Z","shell.execute_reply.started":"2022-07-31T16:20:24.115081Z","shell.execute_reply":"2022-07-31T16:20:24.120376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir_old = test_dir\ntest_dir_new = '../test_data'\nprint(test_dir_old)\nprint(test_dir_new)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:27.786337Z","iopub.execute_input":"2022-07-31T16:20:27.787079Z","iopub.status.idle":"2022-07-31T16:20:27.793554Z","shell.execute_reply.started":"2022-07-31T16:20:27.787018Z","shell.execute_reply":"2022-07-31T16:20:27.792481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir_old","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:30.474355Z","iopub.execute_input":"2022-07-31T16:20:30.475447Z","iopub.status.idle":"2022-07-31T16:20:30.481966Z","shell.execute_reply.started":"2022-07-31T16:20:30.475403Z","shell.execute_reply":"2022-07-31T16:20:30.480876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###Moving file from old test dir to new test dir\nnum_moved = 0\nfor img in os.listdir(test_dir_old+'test'):\n    if img.endswith('.JPG'):\n        shutil.copy(f\"{test_dir_old+'test/'}{img}\", f\"{test_dir_new+'/test/'}{img}\")\n        num_moved += 1\n    else:\n        print('not going to move you!')\nprint(f\"Number of files moved: {num_moved}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:31.455582Z","iopub.execute_input":"2022-07-31T16:20:31.456552Z","iopub.status.idle":"2022-07-31T16:20:41.679824Z","shell.execute_reply.started":"2022-07-31T16:20:31.456505Z","shell.execute_reply":"2022-07-31T16:20:41.678851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir('../test_data/test')) #files have been moved","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:41.682209Z","iopub.execute_input":"2022-07-31T16:20:41.683261Z","iopub.status.idle":"2022-07-31T16:20:41.691406Z","shell.execute_reply.started":"2022-07-31T16:20:41.683220Z","shell.execute_reply":"2022-07-31T16:20:41.690391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing model on test data\ntest = ImageFolder(test_dir_new, transform=transforms.Compose(\n                                        [transforms.Resize([256, 256]),\n                                         transforms.ToTensor()]))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:43.745658Z","iopub.execute_input":"2022-07-31T16:20:43.746337Z","iopub.status.idle":"2022-07-31T16:20:43.760381Z","shell.execute_reply.started":"2022-07-31T16:20:43.746292Z","shell.execute_reply":"2022-07-31T16:20:43.759418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:46.370257Z","iopub.execute_input":"2022-07-31T16:20:46.370870Z","iopub.status.idle":"2022-07-31T16:20:46.378259Z","shell.execute_reply.started":"2022-07-31T16:20:46.370835Z","shell.execute_reply":"2022-07-31T16:20:46.377186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = sorted(os.listdir(test_dir_new + '/test')) # since images in test folder are not in alphabetical order\n#test_images","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:47.162079Z","iopub.execute_input":"2022-07-31T16:20:47.162458Z","iopub.status.idle":"2022-07-31T16:20:47.169471Z","shell.execute_reply.started":"2022-07-31T16:20:47.162425Z","shell.execute_reply":"2022-07-31T16:20:47.168246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image(img, model):\n    \"\"\"Converts image to array and return the predicted class\n        with highest probability\"\"\"\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n\n    return train.classes[preds[0].item()]","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:48.279426Z","iopub.execute_input":"2022-07-31T16:20:48.280146Z","iopub.status.idle":"2022-07-31T16:20:48.286591Z","shell.execute_reply.started":"2022-07-31T16:20:48.280106Z","shell.execute_reply":"2022-07-31T16:20:48.285191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:51.380075Z","iopub.execute_input":"2022-07-31T16:20:51.380927Z","iopub.status.idle":"2022-07-31T16:20:51.389340Z","shell.execute_reply.started":"2022-07-31T16:20:51.380874Z","shell.execute_reply":"2022-07-31T16:20:51.387550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, label = test[0]\nlabel","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:52.172437Z","iopub.execute_input":"2022-07-31T16:20:52.173178Z","iopub.status.idle":"2022-07-31T16:20:52.184871Z","shell.execute_reply.started":"2022-07-31T16:20:52.173134Z","shell.execute_reply":"2022-07-31T16:20:52.183813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(test_images))\nprint(len(test))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:52.988477Z","iopub.execute_input":"2022-07-31T16:20:52.989122Z","iopub.status.idle":"2022-07-31T16:20:52.994827Z","shell.execute_reply.started":"2022-07-31T16:20:52.989082Z","shell.execute_reply":"2022-07-31T16:20:52.993741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting first image\nimg, label = test[-100]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_images[-100], ', Predicted:', predict_image(img, model))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:55.610444Z","iopub.execute_input":"2022-07-31T16:20:55.611120Z","iopub.status.idle":"2022-07-31T16:20:55.819287Z","shell.execute_reply.started":"2022-07-31T16:20:55.611082Z","shell.execute_reply":"2022-07-31T16:20:55.818350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f\"{test_images[0].split('_')[0] + '_'  + test_images[0].split('_')[1]}\"","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:20:59.424687Z","iopub.execute_input":"2022-07-31T16:20:59.425395Z","iopub.status.idle":"2022-07-31T16:20:59.431865Z","shell.execute_reply.started":"2022-07-31T16:20:59.425357Z","shell.execute_reply":"2022-07-31T16:20:59.430800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting all predictions (actual label vs predicted)\nlistt = []\nfor i, (img, label) in enumerate(test):\n    #print('Label:', test_images[i], ', Predicted:', predict_image(img, model))\n    listt.append((f\"{test_images[i].split('_')[0] + '_'+ test_images[i].split('_')[1]}\", predict_image(img, model)))\n    \n#listt","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:21:00.249273Z","iopub.execute_input":"2022-07-31T16:21:00.249998Z","iopub.status.idle":"2022-07-31T16:21:06.940589Z","shell.execute_reply.started":"2022-07-31T16:21:00.249960Z","shell.execute_reply":"2022-07-31T16:21:06.939678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor tup in listt:\n    if tup[0]==tup[1]:\n        count+=1\ntest_accuracy = count/len(listt)*100\nprint(round(test_accuracy, 2))","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:21:18.738062Z","iopub.execute_input":"2022-07-31T16:21:18.738465Z","iopub.status.idle":"2022-07-31T16:21:18.745729Z","shell.execute_reply.started":"2022-07-31T16:21:18.738431Z","shell.execute_reply":"2022-07-31T16:21:18.744683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model #check if the softmax layer is present before you save the model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving to the kaggle working directory ###check this again\nPATH1 = './pt-mdlsd.pth'  \ntorch.save(model.state_dict(), PATH1)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:21:21.566673Z","iopub.execute_input":"2022-07-31T16:21:21.567308Z","iopub.status.idle":"2022-07-31T16:21:21.623882Z","shell.execute_reply.started":"2022-07-31T16:21:21.567271Z","shell.execute_reply":"2022-07-31T16:21:21.622887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH2 = './pt-mdl.pth' \ntorch.save(model, PATH2)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:21:25.640534Z","iopub.execute_input":"2022-07-31T16:21:25.640932Z","iopub.status.idle":"2022-07-31T16:21:25.696878Z","shell.execute_reply.started":"2022-07-31T16:21:25.640898Z","shell.execute_reply":"2022-07-31T16:21:25.695888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking feature importances","metadata":{"execution":{"iopub.status.busy":"2022-07-12T19:16:11.586831Z","iopub.execute_input":"2022-07-12T19:16:11.587587Z","iopub.status.idle":"2022-07-12T19:16:11.605472Z","shell.execute_reply.started":"2022-07-12T19:16:11.587481Z","shell.execute_reply":"2022-07-12T19:16:11.604600Z"}}},{"cell_type":"code","source":"type(train_dl)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-31T16:21:38.449358Z","iopub.execute_input":"2022-07-31T16:21:38.449742Z","iopub.status.idle":"2022-07-31T16:21:38.457146Z","shell.execute_reply.started":"2022-07-31T16:21:38.449707Z","shell.execute_reply":"2022-07-31T16:21:38.455975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Moving data into GPU\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:52:13.444564Z","iopub.execute_input":"2022-07-30T12:52:13.445130Z","iopub.status.idle":"2022-07-30T12:52:13.450559Z","shell.execute_reply.started":"2022-07-30T12:52:13.445089Z","shell.execute_reply":"2022-07-30T12:52:13.449274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader_r = torch.utils.data.DataLoader(test, \n                                            batch_size=batch_size,\n                                            shuffle=True)\n\ntest_loader_r = DeviceDataLoader(test_loader_r, device)\ntest_loader_r","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:52:13.837110Z","iopub.execute_input":"2022-07-30T12:52:13.837794Z","iopub.status.idle":"2022-07-30T12:52:13.845256Z","shell.execute_reply.started":"2022-07-30T12:52:13.837759Z","shell.execute_reply":"2022-07-30T12:52:13.844011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"batch = next(iter(test_loader_r))\nimages, _ = batch\n\nbackground = images[:20]\ntest_images = images[20:24]\n\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(test_images)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T12:31:43.901518Z","iopub.execute_input":"2022-07-19T12:31:43.902242Z","iopub.status.idle":"2022-07-19T12:31:54.221321Z","shell.execute_reply.started":"2022-07-19T12:31:43.902201Z","shell.execute_reply":"2022-07-19T12:31:54.220305Z"}}},{"cell_type":"markdown","source":"shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values] #how can you add the predicted label vs true label to plot\ntest_numpy = np.swapaxes(np.swapaxes(test_images.cpu().numpy(), 1, -1), 1, 2)\n# plot the feature attributions\nshap.image_plot(shap_numpy, -test_numpy)\n#plt.rcParams['figure.figsize'] = [10, 10]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T12:32:02.524092Z","iopub.execute_input":"2022-07-19T12:32:02.524759Z","iopub.status.idle":"2022-07-19T12:32:04.671878Z","shell.execute_reply.started":"2022-07-19T12:32:02.524722Z","shell.execute_reply":"2022-07-19T12:32:04.670944Z"}}},{"cell_type":"code","source":"# since shuffle=True, this is a random sample of test data\nimages, targets =  next(iter(test_loader_r))\nBACKGROUND_SIZE = 20\nbackground_images = images[:BACKGROUND_SIZE]\nbackground_targets = targets[:BACKGROUND_SIZE].cpu().numpy()\n#increase the size after you've fixed everything \n\ntest_images = images[BACKGROUND_SIZE:BACKGROUND_SIZE+9]\ntest_targets = targets[BACKGROUND_SIZE:BACKGROUND_SIZE+9].cpu().numpy()\ndef show_attributions(model):\n    # predict the probabilities of the digits using the test images\n    output = model(test_images.to(device))\n    # get the index of the max log-probability\n    pred = output.max(1, keepdim=True)[1] \n    # convert to numpy only once to save time\n    pred_np = pred.cpu().numpy() \n\n    expl = shap.DeepExplainer(model, background_images)\n    train_classes = ['potato_early', 'potato_healthy', 'potato_late', 'tomato_early', 'tomato_healthy', 'tomato_late'] \n    for i in range(0, len(test_images)):\n        warnings.filterwarnings('ignore')\n        \n        torch.cuda.empty_cache()\n        ti = test_images[[i]]\n        sv = expl.shap_values(ti)\n        sn = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in sv]\n        tn = np.swapaxes(np.swapaxes(ti.cpu().numpy(), 1, -1), 1, 2) #.cpu().numpy()?\n\n        # Prepare the attribution plot, but do not draw it yet\n        # We will add more info to the plots later in the code\n        shap.image_plot(sn, -tn, show=False)\n\n        # Prepare to augment the plot\n        fig = plt.gcf()\n        allaxes = fig.get_axes()\n\n        # Show the actual/predicted class\n        #plot the original image here as well\n        allaxes[0].set_title('Actual: {}, Pred: {}'.format(train_classes[test_targets[i]], train_classes[pred_np[i][0]]), fontsize=10)\n        \n        \n        # Show the probability of each class\n        # There are 11 axes for each picture: 1 for the digit + 10 for each SHAP\n        # There is a last axis for the scale - we don't want to apply a label for that one\n        prob = output[i].detach().cpu().numpy()\n        for x in range(1, len(allaxes)-1):\n            #allaxes[x].set_title('{}'.format(train_classes[x-1]), fontsize=10)\n            allaxes[x].set_title('{}({:.2%})'.format(train_classes[x-1], prob[x-1]), fontsize=10)\n            allaxes[0].imshow(test_images[i].cpu().permute(1, 2, 0))\n#            \n#             allaxes[x].set_title('{}({:.2%})'.format(train_classes[x-1], prob[x-1]), fontsize=9)            \n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:52:18.771389Z","iopub.execute_input":"2022-07-30T12:52:18.771730Z","iopub.status.idle":"2022-07-30T12:52:18.868507Z","shell.execute_reply.started":"2022-07-30T12:52:18.771699Z","shell.execute_reply":"2022-07-30T12:52:18.867508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_attributions = show_attributions(model)\nfeature_attributions","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:52:19.658037Z","iopub.execute_input":"2022-07-30T12:52:19.658394Z","iopub.status.idle":"2022-07-30T12:52:54.273758Z","shell.execute_reply.started":"2022-07-30T12:52:19.658363Z","shell.execute_reply":"2022-07-30T12:52:54.272338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(feature_attributions)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T12:53:35.342706Z","iopub.execute_input":"2022-07-30T12:53:35.343075Z","iopub.status.idle":"2022-07-30T12:53:35.350947Z","shell.execute_reply.started":"2022-07-30T12:53:35.343043Z","shell.execute_reply":"2022-07-30T12:53:35.349926Z"},"trusted":true},"execution_count":null,"outputs":[]}]}