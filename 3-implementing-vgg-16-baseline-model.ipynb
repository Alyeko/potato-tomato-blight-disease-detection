{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains code for the implementation of VGG-16 architecture from the paper 'Image Based Tomato Leaf Disease Detection' based on the network information and values provided by the authors in the paper.\n\nThe VGG-16 architecture is used as a baseline model in my thesis, 'A computer vision approach for the automatic blight disease detection in potato and tomato plants'.","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:09.892400Z","iopub.execute_input":"2022-08-22T13:10:09.892723Z","iopub.status.idle":"2022-08-22T13:10:22.372191Z","shell.execute_reply.started":"2022-08-22T13:10:09.892644Z","shell.execute_reply":"2022-08-22T13:10:22.371060Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os                                      # for working with files\nimport sys\nimport shap                                    # for checking feature importances\nimport torch                                   # Pytorch module \nimport shutil\nimport optuna\nimport warnings\nimport itertools\nimport numpy as np                             # for numerical computationss\nimport pandas as pd                            # for working with dataframes\nimport torch.nn as nn                          # for creating  neural networks\nfrom PIL import Image                          # for checking images\nimport matplotlib.pyplot as plt                # for plotting informations on graph and images using tensors\nimport torch.nn.functional as F                # for functions for calculating loss\nfrom torchsummary import summary               # for getting the summary of our model\nfrom torchvision.models import vgg16\nfrom torchvision.models import resnet50\nfrom torchvision.utils import make_grid        # for data checking\nfrom torch.utils.data import DataLoader        # for dataloaders \nimport torchvision.transforms as transforms    # for transforming images into tensors \nfrom torchvision.datasets import ImageFolder   # for working with classes and images\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:22.374659Z","iopub.execute_input":"2022-08-22T13:10:22.375071Z","iopub.status.idle":"2022-08-22T13:10:26.795128Z","shell.execute_reply.started":"2022-08-22T13:10:22.375026Z","shell.execute_reply":"2022-08-22T13:10:26.794156Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration!","metadata":{}},{"cell_type":"code","source":"os.listdir('/kaggle/input/dataset/idata/Image Dataset/ImageDataset/')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:40.354531Z","iopub.execute_input":"2022-08-22T13:10:40.355222Z","iopub.status.idle":"2022-08-22T13:10:40.372433Z","shell.execute_reply.started":"2022-08-22T13:10:40.355176Z","shell.execute_reply":"2022-08-22T13:10:40.370965Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/dataset/idata/Image Dataset/ImageDataset/'","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:40.591077Z","iopub.execute_input":"2022-08-22T13:10:40.591346Z","iopub.status.idle":"2022-08-22T13:10:40.596784Z","shell.execute_reply.started":"2022-08-22T13:10:40.591321Z","shell.execute_reply":"2022-08-22T13:10:40.595818Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# print(f\"Number of image directories are {len(os.listdir(data_fpath))+len(os.listdir('/kaggle/input/newds/ImageDataset_new/ImageDataset_new/'))}\\n\")\nprint('Number of unique plants are 2, potato and tomato\\n')\nprint('Number of diseases are 4, early and late blight disease for tomato, early and late blight for potato\\n')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:40.788750Z","iopub.execute_input":"2022-08-22T13:10:40.789135Z","iopub.status.idle":"2022-08-22T13:10:40.794745Z","shell.execute_reply.started":"2022-08-22T13:10:40.789102Z","shell.execute_reply":"2022-08-22T13:10:40.793714Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:40.971906Z","iopub.execute_input":"2022-08-22T13:10:40.972637Z","iopub.status.idle":"2022-08-22T13:10:40.979941Z","shell.execute_reply.started":"2022-08-22T13:10:40.972604Z","shell.execute_reply":"2022-08-22T13:10:40.978042Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dir = data_dir + \"train/\"\nvalid_dir = data_dir + \"valid/\"\n# test_dir\ndiseases_tr = os.listdir(train_dir)\ndiseases_va = os.listdir(valid_dir)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:41.830921Z","iopub.execute_input":"2022-08-22T13:10:41.831985Z","iopub.status.idle":"2022-08-22T13:10:41.843813Z","shell.execute_reply.started":"2022-08-22T13:10:41.831940Z","shell.execute_reply":"2022-08-22T13:10:41.842665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"valid_dir","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:42.131411Z","iopub.execute_input":"2022-08-22T13:10:42.131773Z","iopub.status.idle":"2022-08-22T13:10:42.139769Z","shell.execute_reply.started":"2022-08-22T13:10:42.131741Z","shell.execute_reply":"2022-08-22T13:10:42.138568Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"diseases_tr","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:42.526157Z","iopub.execute_input":"2022-08-22T13:10:42.526513Z","iopub.status.idle":"2022-08-22T13:10:42.532873Z","shell.execute_reply.started":"2022-08-22T13:10:42.526483Z","shell.execute_reply":"2022-08-22T13:10:42.531843Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"folder = [i for i in os.listdir(valid_dir)]\nfolder","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:42.831384Z","iopub.execute_input":"2022-08-22T13:10:42.831746Z","iopub.status.idle":"2022-08-22T13:10:42.840390Z","shell.execute_reply.started":"2022-08-22T13:10:42.831714Z","shell.execute_reply":"2022-08-22T13:10:42.839343Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plants = []\nNumberOfDiseases = 0\nfor plant in diseases_tr:\n    if plant.split('___')[0] not in plants:\n        plants.append(plant.split('___')[0])\n    if plant.split('_')[1] != 'healthy':\n        NumberOfDiseases += 1","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:43.903091Z","iopub.execute_input":"2022-08-22T13:10:43.903790Z","iopub.status.idle":"2022-08-22T13:10:43.909610Z","shell.execute_reply.started":"2022-08-22T13:10:43.903752Z","shell.execute_reply":"2022-08-22T13:10:43.908498Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Number of images for each clas in the training data\nnums_train = {}\nfor folder in sorted(os.listdir(f\"{data_dir}/train\")):\n    nums_train[folder] = len(os.listdir(f\"/{data_dir}/train/{folder}\"))\n    \n# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n\nimg_per_training_class = pd.DataFrame(nums_train.values(), index=nums_train.keys(), columns=[\"no. of images\"])\nimg_per_training_class","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:44.695340Z","iopub.execute_input":"2022-08-22T13:10:44.695929Z","iopub.status.idle":"2022-08-22T13:10:45.950518Z","shell.execute_reply.started":"2022-08-22T13:10:44.695894Z","shell.execute_reply":"2022-08-22T13:10:45.949563Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Number of images for each clas in the training data\nnums_valid = {}\nfor folder in sorted(os.listdir(valid_dir)):\n    nums_valid[folder] = len(os.listdir(f\"{valid_dir}{folder}\"))\n    \n# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n\nimg_per_valid_class = pd.DataFrame(nums_valid.values(), index=nums_valid.keys(), columns=[\"no. of images\"])\nimg_per_valid_class","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:49.715498Z","iopub.execute_input":"2022-08-22T13:10:49.715854Z","iopub.status.idle":"2022-08-22T13:10:50.671192Z","shell.execute_reply.started":"2022-08-22T13:10:49.715823Z","shell.execute_reply":"2022-08-22T13:10:50.670043Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#changes the text on matplotlib plots to the computer modern font style\nfrom matplotlib import font_manager\nfont_path = '../input/compumodern/cmu.serif-roman.ttf'\nfont_manager.fontManager.addfont(font_path)\nprop = font_manager.FontProperties(fname=font_path)\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = prop.get_name()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:09:24.343547Z","iopub.execute_input":"2022-08-06T18:09:24.344254Z","iopub.status.idle":"2022-08-06T18:09:24.359531Z","shell.execute_reply.started":"2022-08-06T18:09:24.344217Z","shell.execute_reply":"2022-08-06T18:09:24.358641Z"}}},{"cell_type":"code","source":"# plotting number of images available for each class\nindex = [n for n in range(6)]\nplt.figure(figsize=(8, 6))\nplt.bar(index, [n for n in nums_train.values()], color='#8528B0', width=0.7, align='center')\nplt.xlabel('Classes', fontsize=15)\nplt.ylabel('No of images', fontsize=15)\nplt.xticks(index, [key for key in nums_train.keys()], fontsize=15, rotation=90)\nplt.title('Images per class for training dataset', fontsize=15)\n\n#plt.savefig('/kaggle/working/number_imgs_training.png', dpi=600, bbox_inches=\"tight\")  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:10:55.573380Z","iopub.execute_input":"2022-08-22T13:10:55.573828Z","iopub.status.idle":"2022-08-22T13:10:55.808710Z","shell.execute_reply.started":"2022-08-22T13:10:55.573788Z","shell.execute_reply":"2022-08-22T13:10:55.807711Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# plotting number of images available for each class\nindex = [n for n in range(6)]\nplt.figure(figsize=(8, 6))\nplt.bar(index, [n for n in nums_valid.values()], color='#8528B0', width=0.7) \nplt.xlabel('Classes', fontsize=15)\nplt.ylabel('No of images', fontsize=15)\nplt.xticks(index, [key for key in nums_valid.keys()], fontsize=15, rotation=90)\nplt.title('Images per class for validation dataset', fontsize=15)\n# plt.tight_layout()\n#plt.savefig('/kaggle/working/number_imgs_validation.png', dpi=600, bbox_inches=\"tight\")  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:01.290619Z","iopub.execute_input":"2022-08-22T13:11:01.291237Z","iopub.status.idle":"2022-08-22T13:11:01.498907Z","shell.execute_reply.started":"2022-08-22T13:11:01.291202Z","shell.execute_reply":"2022-08-22T13:11:01.497949Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"The data has already been augmented. see https://github.com/Alyeko/potato-tomato-blight-disease-detection","metadata":{"execution":{"iopub.status.busy":"2022-07-08T19:53:01.18532Z","iopub.execute_input":"2022-07-08T19:53:01.185753Z","iopub.status.idle":"2022-07-08T19:53:01.192473Z","shell.execute_reply.started":"2022-07-08T19:53:01.185709Z","shell.execute_reply":"2022-07-08T19:53:01.191217Z"}}},{"cell_type":"markdown","source":"### Images available for training","metadata":{}},{"cell_type":"code","source":"n_train = 0\nfor value in nums_train.values():\n    n_train += value\nprint(f\"There are {n_train} images for training\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:06.289940Z","iopub.execute_input":"2022-08-22T13:11:06.290734Z","iopub.status.idle":"2022-08-22T13:11:06.296769Z","shell.execute_reply.started":"2022-08-22T13:11:06.290694Z","shell.execute_reply":"2022-08-22T13:11:06.295764Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"n_valid = 0\nfor value in nums_valid.values():\n    n_valid += value\nprint(f\"There are {n_valid} images for validation\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:07.020375Z","iopub.execute_input":"2022-08-22T13:11:07.021051Z","iopub.status.idle":"2022-08-22T13:11:07.026588Z","shell.execute_reply.started":"2022-08-22T13:11:07.020995Z","shell.execute_reply":"2022-08-22T13:11:07.025508Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Checking if here are non img files in the training data folder\n","metadata":{}},{"cell_type":"code","source":"folds = [folder for folder in os.listdir(train_dir)]\nfolds","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:09.459167Z","iopub.execute_input":"2022-08-22T13:11:09.459544Z","iopub.status.idle":"2022-08-22T13:11:09.467812Z","shell.execute_reply.started":"2022-08-22T13:11:09.459511Z","shell.execute_reply":"2022-08-22T13:11:09.466511Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for i in folds:\n    for img in os.listdir(train_dir+i):\n        if not img.endswith('.JPG'):\n            print('yes!')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:09.967738Z","iopub.execute_input":"2022-08-22T13:11:09.968665Z","iopub.status.idle":"2022-08-22T13:11:09.994562Z","shell.execute_reply.started":"2022-08-22T13:11:09.968621Z","shell.execute_reply":"2022-08-22T13:11:09.993664Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for i in folds:\n    for img in os.listdir(valid_dir+i):\n        if not img.endswith('.JPG'):\n            print('yes!')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:11.558055Z","iopub.execute_input":"2022-08-22T13:11:11.558421Z","iopub.status.idle":"2022-08-22T13:11:11.571628Z","shell.execute_reply.started":"2022-08-22T13:11:11.558390Z","shell.execute_reply":"2022-08-22T13:11:11.570737Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:11.816629Z","iopub.execute_input":"2022-08-22T13:11:11.817307Z","iopub.status.idle":"2022-08-22T13:11:11.823303Z","shell.execute_reply.started":"2022-08-22T13:11:11.817270Z","shell.execute_reply":"2022-08-22T13:11:11.822326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {len(os.listdir('/kaggle/input/dataset/idata/Image Dataset/test_data/test'))} images for test\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:13.377784Z","iopub.execute_input":"2022-08-22T13:11:13.378705Z","iopub.status.idle":"2022-08-22T13:11:13.741193Z","shell.execute_reply.started":"2022-08-22T13:11:13.378644Z","shell.execute_reply":"2022-08-22T13:11:13.740175Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(f\"Training dir: {os.listdir('/kaggle/input/dataset/idata/Image Dataset/ImageDataset/')}\")\nprint(f\"All: {os.listdir('/kaggle/input/dataset/idata/Image Dataset')}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:13.742967Z","iopub.execute_input":"2022-08-22T13:11:13.744030Z","iopub.status.idle":"2022-08-22T13:11:13.752362Z","shell.execute_reply.started":"2022-08-22T13:11:13.743971Z","shell.execute_reply":"2022-08-22T13:11:13.751056Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_dir = '/kaggle/input/dataset/idata/Image Dataset/test_data/'\n# print(f\"There are {len(os.listdir('/kaggle/input/newds/ImageDataset_new/ImageDataset_new/test_data'))} images for training\")\nos.listdir(test_dir)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:15.032123Z","iopub.execute_input":"2022-08-22T13:11:15.033147Z","iopub.status.idle":"2022-08-22T13:11:15.043076Z","shell.execute_reply.started":"2022-08-22T13:11:15.033096Z","shell.execute_reply":"2022-08-22T13:11:15.041990Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for img in os.listdir(test_dir+'test'):\n        if not img.endswith('.JPG'):\n            print('Yes! I knew it!')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:15.292822Z","iopub.execute_input":"2022-08-22T13:11:15.293181Z","iopub.status.idle":"2022-08-22T13:11:15.300559Z","shell.execute_reply.started":"2022-08-22T13:11:15.293150Z","shell.execute_reply":"2022-08-22T13:11:15.299284Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation for training ","metadata":{}},{"cell_type":"code","source":"print(train_dir)\nprint(valid_dir)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:20.124677Z","iopub.execute_input":"2022-08-22T13:11:20.125284Z","iopub.status.idle":"2022-08-22T13:11:20.130674Z","shell.execute_reply.started":"2022-08-22T13:11:20.125245Z","shell.execute_reply":"2022-08-22T13:11:20.129563Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"code for for creating the VGG-16 network is adapted from https://www.kaggle.com/code/carloalbertobarbano/vgg16-transfer-learning-pytorch/notebook","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()  \n\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:11:35.568978Z","iopub.execute_input":"2022-08-22T13:11:35.569682Z","iopub.status.idle":"2022-08-22T13:11:35.636155Z","shell.execute_reply.started":"2022-08-22T13:11:35.569645Z","shell.execute_reply":"2022-08-22T13:11:35.635162Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"vgg16 = models.vgg16_bn()\n# vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\nprint(vgg16.classifier[6].out_features) # 1000 \n\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nfeatures = list(vgg16.classifier.children())[:-1] # Remove last layer\nfeatures.extend([nn.Linear(num_features, 6)]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*features) # Replace the model classifier\nprint(vgg16)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:17.385278Z","iopub.execute_input":"2022-08-22T14:07:17.385689Z","iopub.status.idle":"2022-08-22T14:07:19.747267Z","shell.execute_reply.started":"2022-08-22T14:07:17.385643Z","shell.execute_reply":"2022-08-22T14:07:19.746089Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# getting summary of the model\nINPUT_SHAPE = (3, 224, 224)\nprint(summary(vgg16.cuda(), (INPUT_SHAPE)))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:19.749261Z","iopub.execute_input":"2022-08-22T14:07:19.749901Z","iopub.status.idle":"2022-08-22T14:07:19.917695Z","shell.execute_reply.started":"2022-08-22T14:07:19.749861Z","shell.execute_reply":"2022-08-22T14:07:19.916627Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"copy.deepcopy(vgg16.state_dict())","metadata":{"execution":{"iopub.status.busy":"2022-08-21T23:29:19.955438Z","iopub.execute_input":"2022-08-21T23:29:19.955833Z","iopub.status.idle":"2022-08-21T23:29:20.570073Z","shell.execute_reply.started":"2022-08-21T23:29:19.955798Z","shell.execute_reply":"2022-08-21T23:29:20.569146Z"}}},{"cell_type":"code","source":"data_dir","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:24.613217Z","iopub.execute_input":"2022-08-22T14:07:24.613591Z","iopub.status.idle":"2022-08-22T14:07:24.620079Z","shell.execute_reply.started":"2022-08-22T14:07:24.613552Z","shell.execute_reply":"2022-08-22T14:07:24.619146Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=2)\ntorch.save(vgg16.state_dict(), 'VGG16.pt')","metadata":{}},{"cell_type":"markdown","source":"#### The code just below moves images from the test dir to their corresponding test folders ","metadata":{}},{"cell_type":"code","source":"###Creating a new test dir bcause there was an svn file or folder found in the test dir\nos.mkdir('../test_data')\nos.mkdir('../test_data/test')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:26.369823Z","iopub.execute_input":"2022-08-22T13:12:26.370846Z","iopub.status.idle":"2022-08-22T13:12:26.378065Z","shell.execute_reply.started":"2022-08-22T13:12:26.370796Z","shell.execute_reply":"2022-08-22T13:12:26.377162Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_dir_old = test_dir\ntest_dir_new = '../test_data'\nprint(test_dir_old)\nprint(test_dir_new)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:26.625579Z","iopub.execute_input":"2022-08-22T13:12:26.626261Z","iopub.status.idle":"2022-08-22T13:12:26.631388Z","shell.execute_reply.started":"2022-08-22T13:12:26.626226Z","shell.execute_reply":"2022-08-22T13:12:26.630430Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(test_dir_old)\nprint(test_dir_new)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:26.953479Z","iopub.execute_input":"2022-08-22T13:12:26.954046Z","iopub.status.idle":"2022-08-22T13:12:26.960128Z","shell.execute_reply.started":"2022-08-22T13:12:26.953985Z","shell.execute_reply":"2022-08-22T13:12:26.958881Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"os.mkdir('../testy/')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:28.965722Z","iopub.execute_input":"2022-08-22T13:12:28.966158Z","iopub.status.idle":"2022-08-22T13:12:28.972609Z","shell.execute_reply.started":"2022-08-22T13:12:28.966121Z","shell.execute_reply":"2022-08-22T13:12:28.971220Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test_dir_neww = \"../testy/\"\nos.mkdir(test_dir_neww+'test')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:29.163773Z","iopub.execute_input":"2022-08-22T13:12:29.164943Z","iopub.status.idle":"2022-08-22T13:12:29.170718Z","shell.execute_reply.started":"2022-08-22T13:12:29.164888Z","shell.execute_reply":"2022-08-22T13:12:29.169678Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# os.listdir(test_dir_neww+'test') #empty for now\nfolders = [folder for folder in os.listdir(data_dir+'train')] \nfor folder in folders:\n    os.mkdir(f\"{test_dir_neww}test/{folder}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:29.922314Z","iopub.execute_input":"2022-08-22T13:12:29.922682Z","iopub.status.idle":"2022-08-22T13:12:29.935526Z","shell.execute_reply.started":"2022-08-22T13:12:29.922652Z","shell.execute_reply":"2022-08-22T13:12:29.934478Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"os.listdir(test_dir_neww+'test') ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:31.550856Z","iopub.execute_input":"2022-08-22T13:12:31.551541Z","iopub.status.idle":"2022-08-22T13:12:31.558758Z","shell.execute_reply.started":"2022-08-22T13:12:31.551505Z","shell.execute_reply":"2022-08-22T13:12:31.557760Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"###Moving file from old test dir to new test dir\nnum_moved = 0\nfor img in os.listdir(test_dir_old+'/test'):\n    if img.endswith('.JPG'):\n        theclass = img.split('_')[0] + '_' + img.split('.')[0].split('_')[1]\n        #print(theclass)\n        shutil.copy(f\"{test_dir_old+'/test/'}{img}\", f\"{test_dir_neww+'/test/'+theclass+'/'+img}\")\n        num_moved += 1\n    elif img.endswith('svn'):\n        print('not going to move you!')\nprint(f\"Number of files moved: {num_moved}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:31.760043Z","iopub.execute_input":"2022-08-22T13:12:31.761072Z","iopub.status.idle":"2022-08-22T13:12:40.691452Z","shell.execute_reply.started":"2022-08-22T13:12:31.760994Z","shell.execute_reply":"2022-08-22T13:12:40.690508Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(test_dir_neww+'test/'+'potato_late') )","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:42.860515Z","iopub.execute_input":"2022-08-22T13:12:42.860896Z","iopub.status.idle":"2022-08-22T13:12:42.868677Z","shell.execute_reply.started":"2022-08-22T13:12:42.860863Z","shell.execute_reply":"2022-08-22T13:12:42.867394Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"###Moving file from old test dir to new test dir\nnum_moved = 0\nfor img in os.listdir(test_dir_old+'test'):\n    if img.endswith('.JPG'):\n        shutil.copy(f\"{test_dir_old+'test/'}{img}\", f\"{test_dir_new+'/test/'}{img}\")\n        num_moved += 1\n    else:\n        print('not going to move you!')\nprint(f\"Number of files moved: {num_moved}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-18T16:45:45.723763Z","iopub.execute_input":"2022-08-18T16:45:45.724874Z","iopub.status.idle":"2022-08-18T16:45:56.309549Z","shell.execute_reply.started":"2022-08-18T16:45:45.724827Z","shell.execute_reply":"2022-08-18T16:45:56.308414Z"}}},{"cell_type":"code","source":"len(os.listdir('../test_data/test')) #files have been moved","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:44.878403Z","iopub.execute_input":"2022-08-22T13:12:44.878781Z","iopub.status.idle":"2022-08-22T13:12:44.885744Z","shell.execute_reply.started":"2022-08-22T13:12:44.878749Z","shell.execute_reply":"2022-08-22T13:12:44.884793Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"test_images = []\nfor tclass in folders:\n    for img in os.listdir('../testy/test/' +  tclass):\n        test_images.append(img)\n        \ntest_images = sorted(test_images)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:53.956664Z","iopub.execute_input":"2022-08-22T13:12:53.957223Z","iopub.status.idle":"2022-08-22T13:12:53.965754Z","shell.execute_reply.started":"2022-08-22T13:12:53.957186Z","shell.execute_reply":"2022-08-22T13:12:53.964598Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"#Testing model on test data\ntest = ImageFolder(f\"{test_dir_neww}/test\", transform=transforms.Compose(\n                                            [transforms.Resize([256, 256]),\n                                            transforms.ToTensor()]))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:12:54.144257Z","iopub.execute_input":"2022-08-22T13:12:54.145227Z","iopub.status.idle":"2022-08-22T13:12:54.157086Z","shell.execute_reply.started":"2022-08-22T13:12:54.145178Z","shell.execute_reply":"2022-08-22T13:12:54.155944Z"}}},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-08-21T23:57:57.152982Z","iopub.execute_input":"2022-08-21T23:57:57.153355Z","iopub.status.idle":"2022-08-21T23:57:57.160068Z","shell.execute_reply.started":"2022-08-21T23:57:57.153321Z","shell.execute_reply":"2022-08-21T23:57:57.159002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test_images = sorted(os.listdir(test_dir_neww + '/test')) # since images in test folder are not in alphabetical order\n#test_images","metadata":{"execution":{"iopub.status.busy":"2022-08-18T17:44:08.254446Z","iopub.execute_input":"2022-08-18T17:44:08.25481Z","iopub.status.idle":"2022-08-18T17:44:08.260499Z","shell.execute_reply.started":"2022-08-18T17:44:08.25478Z","shell.execute_reply":"2022-08-18T17:44:08.259386Z"}}},{"cell_type":"code","source":"len(test)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T17:51:13.966573Z","iopub.execute_input":"2022-08-18T17:51:13.96723Z","iopub.status.idle":"2022-08-18T17:51:13.974902Z","shell.execute_reply.started":"2022-08-18T17:51:13.967197Z","shell.execute_reply":"2022-08-18T17:51:13.973149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../testy/test')","metadata":{"execution":{"iopub.status.busy":"2022-08-18T17:46:25.439639Z","iopub.execute_input":"2022-08-18T17:46:25.440519Z","iopub.status.idle":"2022-08-18T17:46:25.448143Z","shell.execute_reply.started":"2022-08-18T17:46:25.440485Z","shell.execute_reply":"2022-08-18T17:46:25.447122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir\nTRAIN = 'train'\nVAL = 'valid'\nTEST = 'test'\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])}\n#     TEST: transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#         transforms.ToTensor(),\n#     ])\n# }\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(data_dir, x), \n        transform=data_transforms[x]\n    )\n    for x in [TRAIN, VAL]#, TEST]\n}\n\ntest = ImageFolder(f\"{test_dir_neww}/test\", transform=transforms.Compose(\n                                            [transforms.Resize(256),\n                                             transforms.CenterCrop(224),\n                                             transforms.ToTensor(),]))\n\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=32,\n        shuffle=True, num_workers=4\n    )\n    for x in [TRAIN, VAL]#, TEST]\n}\n\n\ntest_dataloader = torch.utils.data.DataLoader(test, \n                                            batch_size=32,\n                                            shuffle=True, num_workers=4)\n\n# test_loader_r = DeviceDataLoader(test_loader_r, device)\n# test_loader_r\n\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL]}#, TEST]}\ndataset_sizes_test = {'test':len(test)}\n\nfor x in [TRAIN, VAL]:#, TEST]:\n    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n\nprint(f\"Loaded {len(test)} images under test\")\nprint(\"Classes: \")\nclass_names = image_datasets[TRAIN].classes\nprint(image_datasets[TRAIN].classes)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:26.611118Z","iopub.execute_input":"2022-08-22T14:07:26.612267Z","iopub.status.idle":"2022-08-22T14:07:32.487332Z","shell.execute_reply.started":"2022-08-22T14:07:26.612226Z","shell.execute_reply":"2022-08-22T14:07:32.486291Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:32.716058Z","iopub.execute_input":"2022-08-22T14:07:32.716419Z","iopub.status.idle":"2022-08-22T14:07:32.723275Z","shell.execute_reply.started":"2022-08-22T14:07:32.716388Z","shell.execute_reply":"2022-08-22T14:07:32.721933Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\ndef show_databatch(inputs, classes):\n    out = torchvision.utils.make_grid(inputs)\n    imshow(out, title=[class_names[x] for x in classes])\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders[TRAIN]))\nshow_databatch(inputs, classes)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:37.297689Z","iopub.execute_input":"2022-08-22T14:07:37.298766Z","iopub.status.idle":"2022-08-22T14:07:38.703555Z","shell.execute_reply.started":"2022-08-22T14:07:37.298718Z","shell.execute_reply":"2022-08-22T14:07:38.702499Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def visualize_model(vgg16, num_images=6):\n    was_training = vgg16.training\n    \n    # Set model for evaluation\n    vgg16.train(False)\n    vgg16.eval() \n    \n    images_so_far = 0\n\n    for i, data in enumerate(test_dataloader):\n        inputs, labels = data\n        size = inputs.size()[0]\n        \n        if use_gpu:\n            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n        else:\n            inputs, labels = Variable(inputs), Variable(labels)\n        \n        outputs = vgg16(inputs)\n        \n        _, preds = torch.max(outputs.data, 1)\n        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n        \n        print(\"Ground truth:\")\n        show_databatch(inputs.data.cpu(), labels.data.cpu())\n        print(\"Prediction:\")\n        show_databatch(inputs.data.cpu(), predicted_labels)\n        \n        del inputs, labels, outputs, preds, predicted_labels\n        torch.cuda.empty_cache()\n        \n        images_so_far += size\n        if images_so_far >= num_images:\n            break\n        \n    vgg16.train(mode=was_training) # Revert model back to original training state","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:07:42.676292Z","iopub.execute_input":"2022-08-22T14:07:42.677125Z","iopub.status.idle":"2022-08-22T14:07:42.688185Z","shell.execute_reply.started":"2022-08-22T14:07:42.677083Z","shell.execute_reply":"2022-08-22T14:07:42.687171Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def eval_model(vgg16, criterion):\n    since = time.time()\n    avg_loss = 0\n    avg_acc = 0\n    loss_test = 0\n    acc_test = 0\n    \n    test_batches = len(test_dataloader)\n    print(\"Evaluating model\")\n    print('-' * 10)\n    \n    for i, data in enumerate(test_dataloader):\n        if i % 100 == 0:\n            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n\n        vgg16.train(False)\n        vgg16.eval()\n        inputs, labels = data\n\n        if use_gpu:\n            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n        else:\n            inputs, labels = Variable(inputs), Variable(labels)\n\n        outputs = vgg16(inputs)\n\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n\n        loss_test += loss.item()#loss.data[0]\n        acc_test += torch.sum(preds == labels.data)\n\n        del inputs, labels, outputs, preds\n        torch.cuda.empty_cache()\n        \n    avg_loss = loss_test / dataset_sizes_test['test']\n    avg_acc = acc_test / dataset_sizes_test['test']\n    \n    elapsed_time = time.time() - since\n    print()\n    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n    print('-' * 10)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:08:02.388663Z","iopub.execute_input":"2022-08-22T14:08:02.389053Z","iopub.status.idle":"2022-08-22T14:08:02.400738Z","shell.execute_reply.started":"2022-08-22T14:08:02.389012Z","shell.execute_reply":"2022-08-22T14:08:02.399635Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"dataset_sizes_test","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:08:06.593965Z","iopub.execute_input":"2022-08-22T14:08:06.594606Z","iopub.status.idle":"2022-08-22T14:08:06.601608Z","shell.execute_reply.started":"2022-08-22T14:08:06.594570Z","shell.execute_reply":"2022-08-22T14:08:06.600484Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"if use_gpu:\n    vgg16.cuda() #.cuda() will move everything to the GPU side\n    \ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(vgg16.parameters(), lr=0.0005, momentum = 0.9, weight_decay=0.0005)\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:08:07.155195Z","iopub.execute_input":"2022-08-22T14:08:07.155809Z","iopub.status.idle":"2022-08-22T14:08:07.163757Z","shell.execute_reply.started":"2022-08-22T14:08:07.155773Z","shell.execute_reply":"2022-08-22T14:08:07.162735Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"print(\"Test before training\")\neval_model(vgg16, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:08:09.889366Z","iopub.execute_input":"2022-08-22T14:08:09.889993Z","iopub.status.idle":"2022-08-22T14:08:15.491373Z","shell.execute_reply.started":"2022-08-22T14:08:09.889957Z","shell.execute_reply":"2022-08-22T14:08:15.490244Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"visualize_model(vgg16)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:08:28.143745Z","iopub.execute_input":"2022-08-22T14:08:28.144172Z","iopub.status.idle":"2022-08-22T14:08:30.366394Z","shell.execute_reply.started":"2022-08-22T14:08:28.144135Z","shell.execute_reply":"2022-08-22T14:08:30.365222Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"#code adapted from https://www.kaggle.com/code/carloalbertobarbano/vgg16-transfer-learning-pytorch/notebook\ndef train_model(vgg16, criterion, optimizer, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(vgg16.state_dict())\n    best_acc = 0.0\n    \n    avg_loss = 0\n    avg_acc = 0\n    avg_loss_val = 0\n    avg_acc_val = 0\n    \n    train_batches = len(dataloaders[TRAIN])\n    val_batches = len(dataloaders[VAL])\n    \n    train_loss = []\n    train_accuracy = []\n\n    val_loss = []\n    val_accuracy = []\n\n    for epoch in range(num_epochs):\n        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n        print('-' * 10)\n        \n        loss_train = 0\n        loss_val = 0\n        acc_train = 0\n        acc_val = 0\n        \n        vgg16.train(True)\n        \n        for i, data in enumerate(dataloaders[TRAIN]):\n            if i % 100 == 0:\n                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n                \n            # Use half training dataset\n            if i >= train_batches / 2:\n                break\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg16(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            loss_train += loss.item()\n            acc_train += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        print()\n        # * 2 as we only used half of the dataset\n        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n        train_loss.append(avg_loss)\n        \n        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n        train_accuracy.append(avg_acc)\n        \n        vgg16.train(False)\n        vgg16.eval()\n            \n        for i, data in enumerate(dataloaders[VAL]):\n            if i % 100 == 0:\n                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda(), )\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg16(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss_val += loss.item()\n           \n            acc_val += torch.sum(preds == labels.data)\n            \n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        avg_loss_val = loss_val / dataset_sizes[VAL]\n        val_loss.append(avg_loss_val)\n        \n        avg_acc_val = acc_val / dataset_sizes[VAL]\n        val_accuracy.append(avg_acc_val)\n        \n        print(\"Epoch {} result: \".format(epoch))\n        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n        print('-' * 10)\n        print()\n        \n        if avg_acc_val > best_acc:\n            best_acc = avg_acc_val\n            best_model_wts = copy.deepcopy(vgg16.state_dict())\n        \n    elapsed_time = time.time() - since\n    print()\n    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Best acc: {:.4f}\".format(best_acc))\n    \n    vgg16.load_state_dict(best_model_wts)\n    print('train_loss:', train_loss)\n    print('\\n')\n    \n    print('train_accuracy: ', train_accuracy)\n    print('\\n')\n    \n    print('val_loss: ', val_loss)\n    print('\\n')\n    \n    print('val_accuracy: ', val_accuracy)\n    print('\\n')\n    \n    return vgg16","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:14:05.036535Z","iopub.execute_input":"2022-08-22T14:14:05.037032Z","iopub.status.idle":"2022-08-22T14:14:05.069071Z","shell.execute_reply.started":"2022-08-22T14:14:05.036971Z","shell.execute_reply":"2022-08-22T14:14:05.067982Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"vgg16 = train_model(vgg16, criterion, optimizer_ft, num_epochs=30)\ntorch.save(vgg16.state_dict(), '/kaggle/working/VGG16_v1.pt')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:14:05.333635Z","iopub.execute_input":"2022-08-22T14:14:05.334171Z","iopub.status.idle":"2022-08-22T14:55:04.556318Z","shell.execute_reply.started":"2022-08-22T14:14:05.334126Z","shell.execute_reply":"2022-08-22T14:55:04.554866Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"train_loss = [0.02748516672248596, 0.02335947022885543, 0.01974210111572141, 0.01811217683738828, 0.016732405592275703, 0.015311568389775524, 0.014139645742788354, 0.01341035580176459, 0.011762475176252228, 0.011723317290902962, 0.010331566689740867, 0.010638802919779722, 0.009019472943311012, 0.009881487091556061, 0.00911827004924187, 0.008972670997407119, 0.007902410118042236, 0.0077050597658114865, 0.0077709614990459155, 0.006934247788133962, 0.007145934540228399, 0.007054845722529779, 0.006794078764604563, 0.006013630341916211, 0.006228376758471845, 0.005691696616637735, 0.006041063968294149, 0.005835156281312664, 0.005724024996676407, 0.005775055662790189]\n\nval_loss = [0.018836259461005683, 0.02244757811007615, 0.012502466213805836, 0.014264933862233035, 0.012482399723867006, 0.013933113247992497, 0.010542782529190733, 0.010296113010693603, 0.012856502544469189, 0.007498151861559718, 0.008467840510036224, 0.0053892435022599005, 0.006481106927922183, 0.005383909130575061, 0.0072483539003718395, 0.00682119063597776, 0.014194695869521256, 0.009989471186938867, 0.015853084967463637, 0.004023044853760786, 0.004532191415000878, 0.00407760762075176, 0.006640109273409158, 0.004707408756086001, 0.010461586146095572, 0.0053940281744453275, 0.006043445364998823, 0.004451056215451488, 0.0037933143882961256, 0.008085817617974155]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:23:27.581797Z","iopub.execute_input":"2022-08-22T15:23:27.582193Z","iopub.status.idle":"2022-08-22T15:23:27.590444Z","shell.execute_reply.started":"2022-08-22T15:23:27.582157Z","shell.execute_reply":"2022-08-22T15:23:27.589368Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"epochs = [i for i in range(1, 31)]\nplt.plot(epochs, train_loss, '-o', color='blue', label='train_loss')\nplt.plot(epochs, val_loss, '-o', color='green', label='validation_loss')\nplt.xticks(np.arange(min(epochs), max(epochs)+1, 1.0))\nplt.xlabel('Epochs', size=13)\nplt.ylabel('Losses', size=13)\nplt.grid(color='#EAE4E3')\nplt.xticks(rotation=90)\nplt.title('Training and validation losses of VGG16', size=13)\nplt.legend()\nplt.savefig('../working/vgg16-tv-losses.png', dpi=600,  bbox_inches=\"tight\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T16:51:29.309573Z","iopub.execute_input":"2022-08-22T16:51:29.310687Z","iopub.status.idle":"2022-08-22T16:51:31.431892Z","shell.execute_reply.started":"2022-08-22T16:51:29.310645Z","shell.execute_reply":"2022-08-22T16:51:31.430932Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"train_accuracy=  [0.6730, 0.7228, 0.7686, 0.7926,0.8091, 0.8222, 0.8432, 0.8500, 0.8693,0.8712, 0.8857, 0.8775, 0.9000, 0.8922,\n                  0.8962, 0.9043, 0.9143, 0.9160, 0.9146, 0.9275, 0.9223, 0.9273, 0.9278, 0.9383, 0.9316, 0.9410, 0.9318, 0.9376,\n                  0.9390, 0.9358]\n\nvalidation_accuracy = [0.7861, 0.7413, 0.8455, 0.8294, 0.8469, 0.8425, 0.8739, 0.8880, 0.8600, 0.9091,0.8947, 0.9336, 0.9266, 0.9378, \n                       0.9097, 0.9286, 0.8566, 0.8886, 0.8502, 0.9511, 0.9450, 0.9525, 0.9244, .9419, 0.9008, 0.9366, 0.9236, 0.9469, \n                       0.9558, 0.9216, ]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:35:42.064392Z","iopub.execute_input":"2022-08-22T15:35:42.064752Z","iopub.status.idle":"2022-08-22T15:35:42.071840Z","shell.execute_reply.started":"2022-08-22T15:35:42.064721Z","shell.execute_reply":"2022-08-22T15:35:42.070631Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [8, 8]\nepochs = [i for i in range(1, 31)]\nplt.plot(epochs, train_accuracy, '-o', color='blue', label='train_loss')\nplt.plot(epochs, validation_accuracy, '-o', color='green', label='validation_loss')\nplt.xticks(np.arange(min(epochs), max(epochs)+1, 1.0))\nplt.xlabel('Epochs', size=13)\nplt.ylabel('Accuracies', size=13)\nplt.grid(color='#EAE4E3')\nplt.xticks(rotation=90)\nplt.title('Training and validation accuracies of VGG16', size=13)\nplt.legend()\nplt.savefig('../working/vgg16-tv-accuracies.png', dpi=600,  bbox_inches=\"tight\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T16:51:31.992069Z","iopub.execute_input":"2022-08-22T16:51:31.992667Z","iopub.status.idle":"2022-08-22T16:51:34.025869Z","shell.execute_reply.started":"2022-08-22T16:51:31.992632Z","shell.execute_reply":"2022-08-22T16:51:34.024940Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the confusion matrix of the model","metadata":{}},{"cell_type":"code","source":"predictions, targets = [], []  #code adapted from https://stackoverflow.com/questions/63647547/how-to-find-confusion-matrix-and-plot-it-for-image-classifier-in-pytorch\nfor images, labels in test_dataloader:\n    images, labels = images.cuda(), labels.cuda()\n    logps = vgg16(images)\n    output = torch.exp(logps)\n    pred = torch.argmax(output, 1)\n\n    # convert to numpy arrays\n    pred = pred.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    \n    for i in range(len(pred)):\n        predictions.append(pred[i])\n        targets.append(labels[i])","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:46:15.403174Z","iopub.execute_input":"2022-08-22T15:46:15.403565Z","iopub.status.idle":"2022-08-22T15:46:20.060432Z","shell.execute_reply.started":"2022-08-22T15:46:15.403531Z","shell.execute_reply":"2022-08-22T15:46:20.059071Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:52:28.253196Z","iopub.execute_input":"2022-08-22T15:52:28.253773Z","iopub.status.idle":"2022-08-22T15:52:28.309138Z","shell.execute_reply.started":"2022-08-22T15:52:28.253737Z","shell.execute_reply":"2022-08-22T15:52:28.308144Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"vgg16 = vgg16.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:44:49.662331Z","iopub.execute_input":"2022-08-22T15:44:49.662786Z","iopub.status.idle":"2022-08-22T15:44:49.674587Z","shell.execute_reply.started":"2022-08-22T15:44:49.662745Z","shell.execute_reply":"2022-08-22T15:44:49.673571Z"}}},{"cell_type":"code","source":"classes = image_datasets[TRAIN].classes\nclasses","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:49:43.172581Z","iopub.execute_input":"2022-08-22T15:49:43.173204Z","iopub.status.idle":"2022-08-22T15:49:43.186046Z","shell.execute_reply.started":"2022-08-22T15:49:43.173155Z","shell.execute_reply":"2022-08-22T15:49:43.184618Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(targets, predictions)\ncf_matrix","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:52:42.614538Z","iopub.execute_input":"2022-08-22T15:52:42.614917Z","iopub.status.idle":"2022-08-22T15:52:42.627222Z","shell.execute_reply.started":"2022-08-22T15:52:42.614884Z","shell.execute_reply":"2022-08-22T15:52:42.625972Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"class_names = classes\ndataframe = pd.DataFrame(cf_matrix, index=class_names, columns=class_names)\ndataframe","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:53:08.228048Z","iopub.execute_input":"2022-08-22T15:53:08.228408Z","iopub.status.idle":"2022-08-22T15:53:08.243195Z","shell.execute_reply.started":"2022-08-22T15:53:08.228377Z","shell.execute_reply":"2022-08-22T15:53:08.242075Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))  #change size to 4, 4?\n\n#Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=True,cmap=\"OrRd\",fmt=\"d\") #'purples', 'PuRd'\nplt.title(\"Confusion Matrix for VGG16 model\", size=12), plt.tight_layout()\n \nplt.ylabel(\"Actual Class\", size=12), \nplt.xlabel(\"Predicted Class\",  size=12)\nplt.tight_layout()\nplt.savefig('../working/cmatrxvgg16.png', dpi=600,  bbox_inches=\"tight\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T16:07:23.165300Z","iopub.execute_input":"2022-08-22T16:07:23.165675Z","iopub.status.idle":"2022-08-22T16:07:25.206066Z","shell.execute_reply.started":"2022-08-22T16:07:23.165642Z","shell.execute_reply":"2022-08-22T16:07:25.204990Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"print(\"Test Accuracy : {}\".format(accuracy_score(targets, predictions)))\nprint(\"\\nConfusion Matrix : \")\nprint(confusion_matrix(targets, predictions))\nprint(\"\\nClassification Report :\")\nprint(classification_report(targets, predictions, target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T15:54:01.302292Z","iopub.execute_input":"2022-08-22T15:54:01.302738Z","iopub.status.idle":"2022-08-22T15:54:01.337063Z","shell.execute_reply.started":"2022-08-22T15:54:01.302697Z","shell.execute_reply":"2022-08-22T15:54:01.336048Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}